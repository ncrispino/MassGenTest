# MassGen Configuration: Qwen Computer Use (Browser)
#
# This configuration uses Qwen's qwen3-vl-235b-a22b-thinking vision-language model
# for browser automation tasks on the host machine.
#
# Note: The qwen_computer_use tool calls the Qwen API directly (similar to
# how gemini_computer_use calls Gemini directly), so it bypasses MassGen's normal tool flow.
#
# Usage (with visible browser):
#   DISPLAY=:20 massgen --config qwen_computer_use_example.yaml
#
# Usage (headless mode):
#   Set headless: true in environment_config below
#
# Prerequisites:
#   1. Set QWEN_API_KEY in your .env file
#   2. Install Playwright: pip install playwright
#   3. Install browsers: playwright install chromium
#   4. Install OpenAI SDK: pip install openai
#   5. For visible browser: Ensure DISPLAY environment variable is set (e.g., DISPLAY=:20)

agents:
  - id: "qwen_automation_agent"
    backend:
      type: "openai"  # Use a simple backend since tool calls API directly
      model: "gpt-4.1"  # Orchestration model (not used for automation)
      custom_tools:
        - name: ["qwen_computer_use"]
          category: "automation"
          path: "massgen/tool/_qwen_computer_use/qwen_computer_use_tool.py"
          function: ["qwen_computer_use"]
          description: >
            Automate browser tasks using Qwen's qwen3-vl-235b-a22b-thinking vision-language model.
            This tool directly calls the Qwen API with vision capabilities to control the browser.

            The Qwen VL model analyzes screenshots and generates actions to autonomously control 
            the browser to complete tasks. Supports clicking, typing, scrolling, navigation, and more.

            Use this for complex browser automation tasks that require multiple steps and visual understanding.
          preset_args:
            environment: "browser"
            display_width: 1440
            display_height: 900
            max_iterations: 25
            model: "qwen3-vl-235b-a22b-thinking"
            environment_config:
              headless: false  # Set to true for headless mode, false for visible browser
              browser_type: "chromium"  # chromium, firefox, or webkit
        - name: ["understand_image"]
          category: "multimodal"
          path: "massgen/tool/_multimodal_tools/understand_image.py"
          function: ["understand_image"]

    system_message: |
      You are an AI assistant with access to Qwen's qwen3-vl-235b-a22b-thinking vision-language model
      for browser automation on the host machine.

      You have the qwen_computer_use tool which allows you to automate browser tasks.
      This tool internally uses Qwen's VL model to analyze screenshots and generate actions
      to autonomously control a Chromium browser with visible display (if DISPLAY is set) or headless mode.

      **Your capabilities:**
      - Navigate to websites and search for information
      - Click on links, buttons, and UI elements
      - Type into forms and input fields
      - Scroll and navigate through pages
      - Extract information from web pages
      - Complete multi-step web workflows

      **Environment details:**
      - Browser: Chromium (Playwright)
      - Display: Visible or headless (configured via environment_config)
      - Viewport: 1440x900 pixels
      - Max iterations: 25 steps per task
      - Model: qwen3-vl-235b-a22b-thinking (vision-language model with thinking)

      When users ask you to:
      - Browse websites or search for information
      - Fill out forms or interact with web pages
      - Research products or compare options
      - Extract data from multiple pages
      - Navigate complex websites

      Use the qwen_computer_use tool with a clear task description.

      **Example usage:**
      - qwen_computer_use("Search Google for Python 3.12 new features and summarize")
      - qwen_computer_use("Go to Wikipedia and find information about quantum computing")
      - qwen_computer_use("Navigate to GitHub and find popular Rust projects")

      The tool will autonomously complete the task using Qwen's vision-language model and return the results.
      If running with visible browser, you can monitor the automation in real-time.

      **Important notes:**
      - Qwen VL model analyzes screenshots to understand the page
      - Actions are generated based on visual understanding
      - The model provides reasoning (thoughts) at each step
      - Cost-effective compared to other vision-guided automation

display:
  type: "rich_terminal"
